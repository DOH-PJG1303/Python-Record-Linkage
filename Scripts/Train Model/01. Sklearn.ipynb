{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn\n",
    "---------\n",
    "\n",
    "### Author Information\n",
    "**Author:** PJ Gibson  \n",
    "**Email:** Peter.Gibson@doh.wa.gov  \n",
    "**Github:**   https://github.com/DOH-PJG1303\n",
    "\n",
    "### Project Information\n",
    "**Created Date:** 2023-08-09  \n",
    "**Last Updated:** 2023-08-09  \n",
    "**Version:** 1  \n",
    "\n",
    "### Description\n",
    "\n",
    "In this notebook, we train a ML model for record linkage.\n",
    "We'll use sklearn's multi layer perceptron (MLP) model.\n",
    "This is a neural network model that has performed well for a previous RL task of mine.\n",
    "\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "*\\*If you are unfamiliar with the origins of this synthetic data, please see the [Synthetic-Gold](https://github.com/DOH-PJG1303/Synthetic-Gold) github project. We ran the simulation for the state of Nebraska, so all data is relevant to that state.\n",
    "To manage the size of the data we'll have publicly stored on Github, we only captured relevant data for each table for the population living in years 2019-2022*\n",
    "\n",
    "\n",
    "*\\*\\*Annotation improved with the help of chat-GPT*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Supporting libs\n",
    "import time\n",
    "\n",
    "# MLFlow specific libs\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from mlflow.models.signature import infer_signature\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Sci-kit learn specific libs\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for code reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prep Data\n",
    "\n",
    "### 2.1 Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../../Data/Training/04. Training Data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('label',axis=1), df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ML Model\n",
    "\n",
    "### 3.1 Define Param Grid Search Space\n",
    "\n",
    "Since we'll be using a sklearn.neural_network.MLPClassifier, please review [sklearn's documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) to understand the various parameters we can tweak for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the possible sizes for the hidden layers in the neural network. Each tuple represents a \n",
    "# different network architecture. For example, (100,) means one hidden layer with 100 neurons,\n",
    "# (50,2) means two hidden layers with 50 neurons each, etc.\n",
    "hidden_layer_sizes = [(100,), (50,2), (100,2), (50,3),(10,5)]\n",
    "\n",
    "# Define the possible activation functions for the neurons in the network. 'tanh' and 'relu' are\n",
    "# two common choices.\n",
    "activation = ['tanh', 'relu']\n",
    "\n",
    "# Define the possible solvers for weight optimization in the network. 'sgd' stands for Stochastic \n",
    "# Gradient Descent, and 'adam' is a method that computes adaptive learning rates for each weight.\n",
    "solver = ['sgd', 'adam']\n",
    "\n",
    "# Define the possible values for alpha, the regularization parameter in the MLPClassifier. This\n",
    "# parameter helps prevent overfitting by constraining the size of the weights.\n",
    "alpha = [0.0001, 0.05]\n",
    "\n",
    "# Define the possible learning rate schedules for weight updates. 'constant' means the learning \n",
    "# rate stays the same throughout training, and 'adaptive' means the learning rate decreases \n",
    "# whenever progress on the training set stalls.\n",
    "learning_rate = ['constant','adaptive']\n",
    "\n",
    "# Consolidate all these lists into one list, which forms a grid of hyperparameters to be explored.\n",
    "# This will be used later to randomly select a set of hyperparameters for each run of the model.\n",
    "param_grid = [ hidden_layer_sizes , activation, solver, alpha, learning_rate ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MLFlow config\n",
    "\n",
    "This might look complex with the class and all, but it is far from that.\n",
    "The only reason we set up the class is so that we can use a sklearn model in our mlflow log and output a prediction as a probability, not a label.\n",
    "\n",
    "#### 3.2.1 Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment name\n",
    "experiment_name = 'Record Linkage ML Model Training'\n",
    "\n",
    "try:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set the experiment ID\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Define Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming it Iter1 (Iteration 1), 12 linking fields (to capture number raw data fields required)\n",
    "# [fname, mname, lname, dob, sex, add, zip, city, county, state, phone, email]\n",
    "run_name = 'Iter1, 12 linking fields'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Define wrapper/signature\n",
    "\n",
    "The `SklearnModelWrapper` class is a special piece of code that we use to make our machine learning model work well with a tool called MLflow. Let's break down what it does and why we use it:\n",
    "\n",
    "<b><u>What It Does</b></u>\n",
    "\n",
    "- **Wraps Around Our Model**: Think of this class as a special box that holds our machine learning model. It adds some extra features that make the model compatible with MLflow.\n",
    "\n",
    "- **Customizes Predictions**: Inside this box, we can change how the model makes predictions. In our case, we want the model to tell us the probability of a particular outcome, so we've adjusted the prediction method to do just that.\n",
    "\n",
    "<b><u>Why we use it</b></u>\n",
    "\n",
    "- **Works with MLflow**: MLflow is a tool that helps manage machine learning projects. By using this wrapper, our model can easily be used with MLflow, making things like tracking and sharing the model much easier.\n",
    "\n",
    "- **Makes Things Flexible**: By putting our model in this special box, we can quickly change how it works without affecting the rest of our code. If we want to use a different model or make a different prediction, we can do so easily.\n",
    "\n",
    "- **Keeps It Simple**: Even though it might look a bit complex, this wrapper actually makes our code simpler. It takes care of some technical details, so we don't have to worry about them in other parts of our code.\n",
    "\n",
    "\n",
    "The `SklearnModelWrapper` class is like a special adapter for our machine learning model. It allows us to use the model with MLflow and gives us the freedom to customize how the model makes predictions. It's a handy tool that makes our code more flexible and easier to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"A wrapper class for sklearn models that implements the mlflow.pyfunc.PythonModel interface.\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        return self.model.predict_proba(model_input)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Define Metrics Log\n",
    "\n",
    "We created the log_metrics() function to streamline the process of tracking important information about our model's performance. By packaging the logging code into a separate function, we enhance code readability and maintainability, making it easier to modify or extend in the future. This organized approach ensures that our code follows good practices, promoting clarity and efficiency, especially when working with complex machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(y_test, pred_proba_test, predict_binary_test):\n",
    "    \"\"\"Log various performance metrics.\"\"\"\n",
    "\n",
    "    test_auc_score = roc_auc_score(y_test, pred_proba_test)\n",
    "    mlflow.log_metric('auc', test_auc_score)\n",
    "    test_f1_score = f1_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('f1', test_f1_score)\n",
    "    test_accuracy = accuracy_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('accuracy', test_accuracy)\n",
    "    test_precision = precision_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('precision', test_precision)\n",
    "    test_recall = recall_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('recall', test_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Conduct Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_run(run_name, param_grid, n_runs, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Perform a series of runs in MLflow with the given parameters, looping through the hyperparameter grid.\n",
    "\n",
    "    Args:\n",
    "        run_name (str): The name of the run, to be displayed in MLflow.\n",
    "        param_grid (list of lists): The hyperparameter grid containing lists of possible values for each hyperparameter.\n",
    "        n_runs (int): The number of runs to be performed.\n",
    "        X_train (array-like): The training feature data.\n",
    "        y_train (array-like): The training target data.\n",
    "        X_test (array-like): The testing feature data.\n",
    "        y_test (array-like): The testing target data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    This function loops through the hyperparameter grid, selecting random parameters for each run, trains the model, and logs metrics to MLflow. It also wraps the model using the SklearnModelWrapper class, then logs the model to MLflow.\n",
    "    \"\"\"\n",
    "\n",
    "    # With MLflow autologging, hyperparameters and the trained model are automatically logged to MLflow.\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    # Looping through the hyperparameter grid\n",
    "    for i in np.arange(1, n_runs+1):\n",
    "\n",
    "        # Choose current run parameters randomly\n",
    "        cur_run_params = [random.choice(param) for param in param_grid]\n",
    "\n",
    "        # Start a new run in MLflow, giving it a specific name.\n",
    "        with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "            # Training, prediction, and logging metrics\n",
    "            model = MLPClassifier(hidden_layer_sizes=cur_run_params[0],\n",
    "                                  activation=cur_run_params[1],\n",
    "                                  solver=cur_run_params[2],\n",
    "                                  alpha=cur_run_params[3],\n",
    "                                  learning_rate=cur_run_params[4])\n",
    "            \n",
    "            # Log the parameter \n",
    "            mlflow.log_param('Model Type', 'Sklearn-MLPClassifier')\n",
    "\n",
    "            # Fit model to training data, track time it took\n",
    "            fit_start = time.time()\n",
    "            model.fit(X_train, y_train)\n",
    "            fit_end = time.time()\n",
    "            mlflow.log_metric('fit_time', fit_end - fit_start)\n",
    "\n",
    "            # Predict on testing data, track time it took\n",
    "            predict_start = time.time()\n",
    "            pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "            predict_end = time.time()\n",
    "            mlflow.log_metric('predict_time', predict_end - predict_start)\n",
    "\n",
    "            # Log various metrics\n",
    "            log_metrics(y_test, pred_proba_test, model.predict(X_test))\n",
    "\n",
    "            # Wrap and log the model\n",
    "            wrapped_model = SklearnModelWrapper(model)\n",
    "            signature = infer_signature(X_train, wrapped_model.predict(None, X_train))\n",
    "            mlflow.pyfunc.log_model(run_name, python_model=wrapped_model, signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how many runs we want.  Do not exceed number of combinations of param_grid for repetition's sake\n",
    "n_runs = 20\n",
    "\n",
    "# Perform the runs!\n",
    "perform_run(run_name, param_grid, n_runs, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Inspect Output\n",
    "\n",
    "Open a CMD terminal.  Navigate to this directory. If you're starting in the root repo folder, this will look like:\n",
    "\n",
    "```cmd\n",
    "cd \"Scripts\\Train Model\"\n",
    "```\n",
    "\n",
    "Then navigate into the MLFlow UI by typing:\n",
    "\n",
    "```cmd\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "There you can interact with models, compare different models, and select one to register."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
