{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "---------\n",
    "\n",
    "### Author Information\n",
    "**Author:** PJ Gibson  \n",
    "**Email:** Peter.Gibson@doh.wa.gov  \n",
    "**Github:**   https://github.com/DOH-PJG1303\n",
    "\n",
    "### Project Information\n",
    "**Created Date:** 2023-08-09  \n",
    "**Last Updated:** 2023-08-09  \n",
    "**Version:** 1  \n",
    "\n",
    "### Description\n",
    "\n",
    "In this notebook, we train a ML model for record linkage.\n",
    "We'll use PyTorch.\n",
    "\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "*\\*If you are unfamiliar with the origins of this synthetic data, please see the [Synthetic-Gold](https://github.com/DOH-PJG1303/Synthetic-Gold) github project. We ran the simulation for the state of Nebraska, so all data is relevant to that state.\n",
    "To manage the size of the data we'll have publicly stored on Github, we only captured relevant data for each table for the population living in years 2019-2022*\n",
    "\n",
    "\n",
    "*\\*\\*Annotation improved with the help of chat-GPT*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for code reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prep Data\n",
    "\n",
    "### 2.1 Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../../Data/Training/04. Training Data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('label',axis=1), df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TensorFlow\n",
    "\n",
    "### 3.1 Conversion\n",
    "\n",
    "Can't have it resting as a pandas dataframe.\n",
    "Needs to be tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values.astype('float32'))\n",
    "y_train_tensor = torch.tensor(y_train.values.astype('float32'))\n",
    "X_test_tensor = torch.tensor(X_test.values.astype('float32'))\n",
    "y_test_tensor = torch.tensor(y_test.values.astype('float32'))\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders (you can adjust the batch_size and shuffle as needed)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define Param Grid Search Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    [64, 128, 256],               # Hidden dimensions\n",
    "    [0.01, 0.001, 0.0001],         # Learning rates\n",
    "    [10, 20, 30]                   # Number of epochs\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Wrapping/Defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"A wrapper class for PyTorch models that implements the mlflow.pyfunc.PythonModel interface.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def predict(self, context, model_input):\n",
    "        with torch.no_grad():\n",
    "            model_input_tensor = torch.tensor(model_input.values, dtype=torch.float32)\n",
    "            output = self.model(model_input_tensor)\n",
    "            return output.numpy()[:, 0]\n",
    "\n",
    "# Define a MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 MLFlow config\n",
    "\n",
    "### 4.1 Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment name\n",
    "experiment_name = 'Record Linkage ML Model Training'\n",
    "\n",
    "try:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set the experiment ID\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming it Iter1 (Iteration 1), 12 linking fields (to capture number raw data fields required)\n",
    "# [fname, mname, lname, dob, sex, add, zip, city, county, state, phone, email]\n",
    "run_name = 'Iter1, 12 linking fields'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Log Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(y_test, pred_proba_test):\n",
    "    \"\"\"Log various performance metrics.\"\"\"\n",
    "\n",
    "    test_auc_score = roc_auc_score(y_test, pred_proba_test)\n",
    "    mlflow.log_metric('auc', test_auc_score)\n",
    "    predict_binary_test = (pred_proba_test > 0.5).astype(int)\n",
    "    test_f1_score = f1_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('f1', test_f1_score)\n",
    "    test_accuracy = accuracy_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('accuracy', test_accuracy)\n",
    "    test_precision = precision_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('precision', test_precision)\n",
    "    test_recall = recall_score(y_test, predict_binary_test)\n",
    "    mlflow.log_metric('recall', test_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform runs\n",
    "\n",
    "def perform_run(run_name, param_grid, n_runs, train_loader, test_loader):\n",
    "\n",
    "    input_size = train_loader.dataset.tensors[0].size(1)\n",
    "    mlflow.pytorch.autolog()\n",
    "\n",
    "    for i in range(n_runs):\n",
    "\n",
    "        hidden_size, lr, epochs = [random.choice(param) for param in param_grid]\n",
    "\n",
    "        with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "            # Log the parameter \n",
    "            mlflow.log_param('Model Type', 'Pytorch-MLPClassifier')\n",
    "\n",
    "            # For whatever reason, mlflow pytorch autologging doesn't capture these.\n",
    "            mlflow.log_param('learning_rate',lr)\n",
    "            mlflow.log_param('hidden_layer_sizes',hidden_size)\n",
    "            mlflow.log_param('num_epochs', epochs)\n",
    "\n",
    "            model = MLP(input_size, hidden_size)\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            # Training\n",
    "            for epoch in range(epochs): # You may adjust the number of epochs\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs.squeeze(), batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # Testing and logging metrics\n",
    "            pred_proba_test = []\n",
    "            with torch.no_grad():\n",
    "                for batch_X, _ in test_loader:\n",
    "                    outputs = model(batch_X)\n",
    "                    pred_proba_test.extend(torch.sigmoid(outputs.squeeze()).numpy())\n",
    "            pred_proba_test = np.array(pred_proba_test)\n",
    "\n",
    "            log_metrics(test_loader.dataset.tensors[1].numpy(), pred_proba_test)\n",
    "\n",
    "            # Logging model to MLflow\n",
    "            mlflow.pytorch.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how many runs we want.  Do not exceed number of combinations of param_grid for repetition's sake\n",
    "n_runs = 20\n",
    "\n",
    "# Perform the runs!\n",
    "perform_run(run_name, param_grid, n_runs, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Inspect Output\n",
    "\n",
    "Open a CMD terminal.  Navigate to this directory. If you're starting in the root repo folder, this will look like:\n",
    "\n",
    "```cmd\n",
    "cd \"Scripts\\Train Model\"\n",
    "```\n",
    "\n",
    "Then navigate into the MLFlow UI by typing:\n",
    "\n",
    "```cmd\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "There you can interact with models, compare different models, and select one to register."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_loader.dataset.tensors[0].size(1)\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "hidden_size, lr, epochs = [random.choice(param) for param in param_grid]\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "    # Log the parameter \n",
    "    mlflow.log_param('Model Type', 'Pytorch-MLPClassifier')\n",
    "\n",
    "    model = MLP(input_size, hidden_size)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(epochs): # You may adjust the number of epochs\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Testing and logging metrics\n",
    "    pred_proba_test = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            pred_proba_test.extend(torch.sigmoid(outputs.squeeze()).numpy())\n",
    "    pred_proba_test = np.array(pred_proba_test)\n",
    "\n",
    "    log_metrics(test_loader.dataset.tensors[1].numpy(), pred_proba_test)\n",
    "\n",
    "    # # Logging model to MLflow\n",
    "    # mlflow.pytorch.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
