{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join and Wrangle\n",
    "---------\n",
    "\n",
    "### Author Information\n",
    "**Author:** PJ Gibson  \n",
    "**Email:** Peter.Gibson@doh.wa.gov  \n",
    "**Github:**   https://github.com/DOH-PJG1303\n",
    "\n",
    "### Project Information\n",
    "**Created Date:** 2023-08-04  \n",
    "**Last Updated:** 2023-08-08  \n",
    "**Version:** 1  \n",
    "\n",
    "### Description\n",
    "To start off, we'll be wrangling raw data output from the [Synthetic-Gold](https://github.com/DOH-PJG1303/Synthetic-Gold) github project.\n",
    "Please explore that documentation if you are curious about the project and the underlying data.\n",
    "We ran the simulation for the state of Nebraska, so all data is relevant to that state.\n",
    "To manage the size of the data we'll have publicly stored on Github, we only captured relevant data for each table for the population living in years 2019-2022\n",
    "\n",
    "1. We'll combine raw data tables from the synthetic gold database to extract as much information as we might want.\n",
    "    - **NOTE:** We've filtered the data to only people born in \"Hall County, NE\" to reduce the volume size.  Most computers wouldn't be able to handle the computation of much larger datasets.\n",
    "2. Then we'll intentionally tweak some fields to add congregate living facility phone numbers as well as nicknames for a subset of the population.  These serve as extra fields to then be called on when we apply messiness to data. \n",
    "\n",
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our \"long-formatted\" data.  Note only years 2019-2022\n",
    "df_long = pd.read_parquet('../../Data/SyntheticGold/LongSyntheticGold')\n",
    "\n",
    "# Load in births data, specifically to catch parents_partnership_id as well as status as twin/triplet/jr\n",
    "df_births = pd.read_parquet('../../Data/SyntheticGold/Births')[['ssn','parents_partnership_id','is_twin','is_triplet','is_jr']]\n",
    "\n",
    "# Read in housing events to track births (event=1)\n",
    "df_housingEvents = pd.read_parquet('../../Data/SyntheticGold/HousingEvents').query('event_type == 1')[['ssn','new_house_id']]\n",
    "\n",
    "# Read in email events to track secondary email\n",
    "df_emailEvents = pd.read_parquet('../../Data/SyntheticGold/EmailEvents')[['ssn','secondary_email','event_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in housing lookup.  We'll later divide this into 2 dataframes\n",
    "df_housingLookup = pd.read_parquet('../../Data/SyntheticGold/HousingLookup')\n",
    "\n",
    "# Get a list of houses in Hall County\n",
    "df_county_houses = df_housingLookup.query('county_name == \"Hall County\"')[['house_id']]\n",
    "\n",
    "# Filter down our columns\n",
    "df_housingLookup = df_housingLookup[['house_id','building_id','num_units']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nicknames = pd.read_csv('../../Data/SyntheticGold/Nicknames.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Join Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find SSNs belonging to all people born in \"Hall County\"\n",
    "df = pd.merge(df_housingEvents, df_county_houses, left_on='new_house_id', right_on='house_id', how='inner')[['ssn']]\n",
    "\n",
    "# Collect long-formatted synthetic data for all those people\n",
    "df = pd.merge(df, df_long, on='ssn', how='inner')\n",
    "\n",
    "# Add \"building_id\",\"num_units\" columns to existing data\n",
    "df = pd.merge(df, df_housingLookup, on='house_id', how='inner')\n",
    "\n",
    "# Add 'parents_partnership_id','is_twin','is_triplet','is_jr' columns to existing data\n",
    "df = pd.merge(df, df_births, on='ssn', how='inner')\n",
    "\n",
    "# Add 'secondary_email','event_year' columns to existing data\n",
    "df = pd.merge(df, df_emailEvents, on='ssn', how='left')\n",
    "\n",
    "# Only provide secondary email for year's where that data would have been available\n",
    "mask_email = df['event_year'] > df['year']\n",
    "df.loc[mask_email,'secondary_email'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selective Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Congregate phone numbers\n",
    "\n",
    "Based on the constraints I had while creating the synthetic society, phone numbers serve as very good unique identifiers for people.\n",
    "Because of this, I'd like to create some logic that makes phone numbers less of unique identifiers and more representative of what we'd see in real life.\n",
    "To do this, we'll do the following:\n",
    "\n",
    "1. If you live in a building with >75 units, you get the same landline phone number (taken from one person within building, smallest phone number numerically (sorted ascending)).\n",
    "2. A person's phone number is taken as `COALESCE( congregate_landline, mobile_phone, landline )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every building with 75+ units, get 1 representative unit's landline in a dataframe with ['building_id','phone_landline_congregate']\n",
    "df_congregate_facilities = df[df['num_units'] > 75]\\\n",
    "                                .sort_values('phone_landline', na_position='last')\\\n",
    "                                .groupby('building_id')\\\n",
    "                                .first()\\\n",
    "                                .rename(columns={'phone_landline':'phone_landline_congregate'})\\\n",
    "                                .reset_index()\\\n",
    "                                [['building_id','phone_landline_congregate']]\n",
    "\n",
    "# Join up the data\n",
    "df = pd.merge(df, df_congregate_facilities, on='building_id', how='left')\n",
    "\n",
    "# Assign phone number\n",
    "df = df.assign(phone = df[['phone_landline_congregate', 'phone_mobile', 'phone_landline']].bfill(axis=1).iloc[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Nicknames\n",
    "\n",
    "We want to give nicknames to some percent of the population.\n",
    "For reproducibility's sake, I chose the smallest SSN value per group of offspring (share same parents).\n",
    "At one point this represented the first born, but for all births post-2011, this might not be the case.\n",
    "\n",
    "For this specific data, around 12.2% of the population will have a nickname.\n",
    "This nickname is only activated by the nickname() function within the \"DirtyFunctions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get name,ssn of one person per family (smallest SSN value)\n",
    "df_firstborns = df.assign(dedupe_col1=df.groupby('parents_partnership_id')['ssn'].rank(method='min'))\\\n",
    "                    .query('dedupe_col1 == 1')\\\n",
    "                    .drop(['dedupe_col1'], axis=1)\\\n",
    "                    .drop_duplicates(subset=['ssn'])\\\n",
    "                    [['ssn','first_name']]\n",
    "\n",
    "# Assign all relevant firstname nicknames, random shuffle (w/ random state), and drop duplicates to randomly select one nickname when multiple available.\n",
    "df_firstborn_nicknames = pd.merge(df_firstborns, df_nicknames, on='first_name', how='left')\\\n",
    "                           .sample(frac=1, random_state=42)\\\n",
    "                           .drop_duplicates(subset=['ssn'])\\\n",
    "                           [['ssn','nickname']]\n",
    "\n",
    "# Assign those nicknames\n",
    "df = pd.merge(df, df_firstborn_nicknames, on='ssn', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(unique_id = np.arange(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Person ID\n",
    "\n",
    "Since we'll be using the SSN field as a field for linking, we should assign a copy of the SSN value (untouched) as the person_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['person_id'] = df['ssn'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('../../Data/Training/01. Wrangled Clean Data.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
