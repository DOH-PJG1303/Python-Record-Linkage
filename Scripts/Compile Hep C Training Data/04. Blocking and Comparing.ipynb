{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocking and Comparing\n",
    "---------\n",
    "\n",
    "### Author Information\n",
    "**Author:** PJ Gibson  \n",
    "**Email:** Peter.Gibson@doh.wa.gov  \n",
    "**Github:**   https://github.com/DOH-PJG1303\n",
    "\n",
    "### Project Information\n",
    "**Created Date:** 2023-08-09  \n",
    "**Last Updated:** 2023-08-09  \n",
    "**Version:** 1  \n",
    "\n",
    "### Description\n",
    "\n",
    "In this notebook, we'll be blocking our data together.\n",
    "This limits the number of pairs by removing many extremely disparate pairs and hopefully far less matching pairs.\n",
    "See [this paper](https://usc-isi-i2.github.io/papers/michelson06-aaai.pdf) for more detail on blocking schemas.\n",
    "\n",
    "Then we'll compare fields using established functions along with custom functions.\n",
    "Here's where you get to be creative in crafting your feature columns.\n",
    "\n",
    "Both of these steps are imperative to record linkage.\n",
    "Note that if you want your model to work properly, you also need to apply this **exact** same step to your applied data.\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "*\\*If you are unfamiliar with the origins of this synthetic data, please see the [Synthetic-Gold](https://github.com/DOH-PJG1303/Synthetic-Gold) github project. We ran the simulation for the state of Nebraska, so all data is relevant to that state.\n",
    "To manage the size of the data we'll have publicly stored on Github, we only captured relevant data for each table for the population living in years 2019-2022*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import jellyfish\n",
    "\n",
    "# Record linkage specific resources\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.preprocessing import clean, phonetic\n",
    "from recordlinkage.index import Block\n",
    "from recordlinkage.base import BaseCompareFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in preprocessed data from last file\n",
    "df = pd.read_parquet('../../Data/Training/03. Preprocessed Data Hep C.parquet')\n",
    "\n",
    "# Take a couple of fields from cleaner data to help us with blocking schema\n",
    "df_supplemental = pd.read_parquet('../../Data/Training/01. Wrangled Clean Data Hep C.parquet')[['unique_id','parents_partnership_id','building_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Join extra fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add on those extra 3 columns\n",
    "df = pd.merge(df, df_supplemental, on='unique_id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Blocking\n",
    "\n",
    "Our blocking schema consists of 4 different blocks:\n",
    "\n",
    "* People who have the same DOB\n",
    "    * Captures twins\n",
    "* People who have the same sounding (metaphone) firstname AND lastname\n",
    "    * Captures Jrs (same fname,lname as parent.  Usually father/son)\n",
    "* People who live in the same building\n",
    "    * Similar address for apartments, also set congregate phone-landline for buildings with 75+ people in script 01. \n",
    "* People who have the same parents\n",
    "    * Captures siblings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index (pairs of records to compare)\n",
    "indexer = rl.Index()\n",
    "\n",
    "# Generate a blocking scheme as a union of the following blocks\n",
    "indexer.add(Block('dob'))\n",
    "indexer.add(Block(['meta_fname', 'meta_lname']))\n",
    "indexer.add(Block('building_id'))\n",
    "indexer.add(Block('parents_partnership_id'))\n",
    "\n",
    "pairs = indexer.index(df, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Define customized functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compare_dob_score(BaseCompareFeature):\n",
    "    def _compute_vectorized(self, dob1, dob2):\n",
    "        \n",
    "        # Initialize an empty pandas series to hold the comparison scores\n",
    "        score = pd.Series(np.nan, index=dob1.index)\n",
    "\n",
    "        # If either date is missing, return 0\n",
    "        score[(dob1.isnull()) | (dob2.isnull())] = -1\n",
    "\n",
    "        # Extract the year, month, and day from each date\n",
    "        dob_y_1 = dob1.str[2:4]\n",
    "        dob_y_2 = dob2.str[2:4]\n",
    "\n",
    "        dob_m_1 = dob1.str[5:7]\n",
    "        dob_m_2 = dob2.str[5:7]\n",
    "\n",
    "        dob_d_1 = dob1.str[8:]\n",
    "        dob_d_2 = dob2.str[8:]\n",
    "\n",
    "        # Check whether the year, month, and day are the same for each date\n",
    "        same_y = (dob_y_1 == dob_y_2)\n",
    "        same_m = (dob_m_1 == dob_m_2)\n",
    "        same_d = (dob_d_1 == dob_d_2)\n",
    "\n",
    "        # Check whether the month and day are swapped between the two dates\n",
    "        swap_m_d = (dob_m_1 == dob_d_2) & (dob_d_1 == dob_m_2)\n",
    "        swap_check = (dob_m_1 != dob_m_2)\n",
    "\n",
    "        # If the year is the same and the month and day are swapped, return 2.5/3.0\n",
    "        score[(same_y & swap_m_d & swap_check)] = 2.5/3.0\n",
    "\n",
    "        # Otherwise, return the average of whether the year, month, and day are the same\n",
    "        score[(~same_y | ~swap_m_d | ~swap_check)] = (same_y + same_m + same_d).astype(int) / 3.0\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compare_name_score(BaseCompareFeature):\n",
    "    def _compute_vectorized(self, raw1, sdx1, meta1, raw2, sdx2, meta2):\n",
    "        \n",
    "        # Initialize an empty pandas series to hold the comparison scores\n",
    "        score = pd.Series(np.nan, index=raw1.index)\n",
    "\n",
    "        # If any of the data is missing, return -1.0\n",
    "        score[(raw1.isnull()) | (sdx1.isnull()) | (meta1.isnull()) | (raw2.isnull()) | (sdx2.isnull()) | (meta2.isnull())] = -1\n",
    "\n",
    "        # Check if raw1 is in raw2 or vice versa\n",
    "        out = raw1.combine(raw2, lambda x, y: x in y or y in x if isinstance(x, str) and isinstance(y, str) else False).astype(int)\n",
    "\n",
    "        # Check if sdx1 is equal to sdx2\n",
    "        out += (sdx1 == sdx2).astype(int)\n",
    "\n",
    "        # Check if meta1 is equal to meta2\n",
    "        out += (meta1 == meta2).astype(int)\n",
    "\n",
    "        # Store the average score in the 'score' series\n",
    "        score[~((raw1.isnull()) | (sdx1.isnull()) | (meta1.isnull()) | (raw2.isnull()) | (sdx2.isnull()) | (meta2.isnull()))] = out / 4.0\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compare_custom_dlev_distance(BaseCompareFeature):\n",
    "    def _compute_vectorized(self, s1, s2):\n",
    "        \"\"\"\n",
    "        This function calculates the Damerau-Levenshtein distance between two strings s1 and s2.\n",
    "        It normalizes the distance by the average length of the two strings.\n",
    "        If either string is None, it returns -1.0.\n",
    "        \"\"\"\n",
    "        out = [-1.0 if pd.isnull(x) or pd.isnull(y) \n",
    "               else 1.0 - (jellyfish.damerau_levenshtein_distance(x, y) / ((len(x) + len(y)) / 2.0)) \n",
    "               for x, y in zip(s1, s2)]\n",
    "        return pd.Series(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compare_swapped_fields(BaseCompareFeature):\n",
    "    def _compute_vectorized(self, s1_1, s1_2, s2_1, s2_2):\n",
    "        \"\"\"\n",
    "        This function determines whether or not someone accidentally swapped two fields\n",
    "        \"\"\"\n",
    "        return ((s1_1 == s2_2) & (s1_2 == s2_1) ).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|    | Field1   | Field2   | Method    |\n",
    "|---:|:---------|:---------|:----------|\n",
    "|  0 | fname_1  | fname_2  | jaro_wink |\n",
    "|  1 | fname_1  | fname_2  | dlev_len  |\n",
    "|  2 | mname_1  | mname_2  | is_equal  |\n",
    "|  3 | lname_1  | lname_2  | jaro_wink |\n",
    "|  4 | lname_1  | lname_2  | dlev_len  |\n",
    "|  5 | phone_1  | phone_2  | dlev_len  |\n",
    "|  6 | add_1    | add_2    | jaro_wink |\n",
    "|  7 | add_1    | add_2    | dlev_len  |\n",
    "|  8 | county_1 | county_2 | dlev_len  |\n",
    "|  9 | state_1  | state_2  | is_equal  |\n",
    "| 10 | zip_1    | zip_2    | dlev_len  |\n",
    "| 11 | sex_1    | sex_2    | is_equal  |\n",
    "| 12 | email_1  | email_2  | jaro_wink |\n",
    "| 13 | city_1  | city_2  | jaro_wink |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Compare object\n",
    "compare = rl.Compare()\n",
    "\n",
    "# Perform all of the comparisons described above\n",
    "compare.string('fname','fname','jarowinkler', label='jwink_fname', missing_value=-1.0)\n",
    "compare.add(compare_custom_dlev_distance('fname','fname',label='dlev_fname'))\n",
    "compare.string('lname','lname','jarowinkler',label='jwink_lname', missing_value=-1.0)\n",
    "compare.add(compare_custom_dlev_distance('lname','lname',label='dlev_lname'))\n",
    "compare.add(compare_custom_dlev_distance('phone','phone',label='dlev_phone'))\n",
    "compare.string('add','add','jarowinkler',label='jwink_add', missing_value=-1.0)\n",
    "compare.add(compare_custom_dlev_distance('add','add',label='dlev_add'))\n",
    "compare.add(compare_custom_dlev_distance('county','county',label='dlev_county'))\n",
    "compare.exact('state','state',label='exact_state', missing_value=-1.0)\n",
    "compare.add(compare_custom_dlev_distance('zip','zip',label='dlev_zip'))\n",
    "compare.exact('ssn','ssn', label='exact_ssn',missing_value=-1.0)\n",
    "compare.string('city','city','jarowinkler',label='jwink_city', missing_value=-1.0)\n",
    "compare.add(compare_swapped_fields(('fname','lname'),('fname','lname'),label='swapped_fname_lname'))\n",
    "compare.add(compare_dob_score('dob','dob',label='dob_comparison_score'))\n",
    "compare.add(compare_name_score(('fname','sdx_fname','meta_fname'),('fname','sdx_fname','meta_fname'),label='fname_comparison_score'))\n",
    "compare.add(compare_name_score(('lname','sdx_lname','meta_lname'),('lname','sdx_lname','meta_lname'),label='lname_comparison_score'))\n",
    "\n",
    "# Add our label column\n",
    "compare.exact('person_id','person_id',label='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the comparisons (should take nearly 13 minutes)\n",
    "features = compare.compute(pairs,df,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First remove instances where the rows are the exact same\n",
    "# features = features.loc[features.index.map(lambda x: x[0] != x[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Split by label\n",
    "\n",
    "The label=0 class has far more pairs than the label=1 class.\n",
    "We'll treat them differently when sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = features[features.label==0]\n",
    "df_minority = features[features.label==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Sample \"majority\" class\n",
    "\n",
    "As described above, the label=0 class has the majority of the pairs we've created.\n",
    "We want to sample them in a non-random way.\n",
    "If we were to sample the label=0 data randomly and select 5 rows, we'd see something like this (exaggerated for sample's sake):\n",
    "\n",
    "| fname_comparison_score | dlev_fname | jwink_lname | dob_comparison_score | dlev_phone |\n",
    "|-----------------------|------------|-------------|---------------------|------------|\n",
    "|         low              |      low      |   low          |    medium                 |    low        |\n",
    "|         low              |   low         |    low         |      low               |    medium        |\n",
    "|         low              |    low        |    low         |      low               |    low        |\n",
    "|         low              |    low        |    low         |      medium               |    low        |\n",
    "|         low              |    low        |    low         |      low               |    low        |\n",
    "\n",
    "You'll notice that most of the scores are \"low\".  This is because most of the generated pairs are very easy to determine as non-matches.\n",
    "We're interested in finding the trickier situations for our model training.  That way, it can deal with tricky situations like twins, married people, and passed-down names.\n",
    "\n",
    "To do this, we divide each of the five columns listed above into \"low\",\"medium\",and \"high\" buckets to reflect their scores relative to the rest of the dataset.\n",
    "Let's consider these as 0,1,2 for this example sake.\n",
    "All buckets being low would be reflected as 0_0_0_0_0.\n",
    "All buckets being high would be reflected as 2_2_2_2_2.\n",
    "The way that the code is working is such that every combination between all zeros and all twos is sampled from equally.\n",
    "This way, we get a very non-random sample that contains a wide variety of combinations.\n",
    "\n",
    "#### Note: The following cell can take around 100 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bins = pd.DataFrame()\n",
    "\n",
    "# Discretize the fields into bins\n",
    "for col in ['fname_comparison_score','dlev_fname', 'jwink_lname', 'dob_comparison_score', 'dlev_phone']:\n",
    "    df_bins[col + '_bin'] = pd.qcut(df_majority[col], q=3, duplicates='drop')\n",
    "\n",
    "# Create a 'strata' column that combines the bins\n",
    "df_bins['strata'] = df_bins.apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
    "\n",
    "# Combine the bins back to df_majority\n",
    "df_majority = pd.concat([df_majority, df_bins], axis=1)\n",
    "\n",
    "# Sample from each stratum\n",
    "samples = []\n",
    "for stratum, group in df_majority.groupby('strata'):\n",
    "    samples.append(group.sample(min(len(group), 100000 // df_majority['strata'].nunique()), random_state=42, replace=False))\n",
    "\n",
    "# Concatenate the samples into a single dataframe\n",
    "df_majority_sampled = pd.concat(samples)\n",
    "\n",
    "# Drop the bin and strata columns\n",
    "df_majority_sampled = df_majority_sampled.drop(['fname_comparison_score_bin', 'dlev_fname_bin', 'jwink_lname_bin', 'dob_comparison_score_bin', 'dlev_phone_bin', 'strata'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Sample the minority class (randomly)\n",
    "\n",
    "We're less concerned about specifically sampling this minority class.  So we'll do it randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample minority class so it matches in size with newly-sampled majority class \n",
    "df_minority_sampled = df_minority.loc[df_minority.index.map(lambda x: x[0] != x[1])].sample(len(df_majority_sampled), random_state=42, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Combine minority and majority samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine majority class with upsampled minority class\n",
    "df_resampled = pd.concat([df_majority_sampled, df_minority_sampled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out to a .csv file in our data folder\n",
    "df_resampled.to_parquet('../../Data/Training/04. Training Data Hep C.parquet',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
